{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1e75d7",
   "metadata": {},
   "source": [
    "# Vision Transformers (ViT) & Attention\n",
    "\n",
    "In this lecture, we are going to look at a very recent influential architecture, the so called **Vision Transformer (ViT)**, an adaptation of Transformers (common to NLP tasks) for computer vision problems.\n",
    "\n",
    "Roland Kwitt, 2023\n",
    "\n",
    "\n",
    "<div style=\"padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;\">\n",
    "Credits: much of this implementation is an annotated version of the ViT implementation of Phil Wang on \n",
    "    <a href=\"https://github.com/lucidrains/vit-pytorch\">GitHub</a>.\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The key reference is:\n",
    "\n",
    "[Dosovitskiy21a]     \n",
    "**An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**    \n",
    "Dosovitskiy et al., ICLR 2021    \n",
    "[arXiv](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aff78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed07fe-60c3-42a8-b2c5-82e4f8790ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"./Dosovitskiy21a.pdf\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737f4af-2788-4a41-9aa7-b8f2f16ba436",
   "metadata": {},
   "source": [
    "Further, we will have to take a closer look at the seminal *Attention is all you need* paper, as the attention mechanism introduced there is an essential component the (Vision) Transformer architecture.\n",
    "\n",
    "[Vaswani17a]    \n",
    "**Attention is all you need**    \n",
    "Vaswani et al., NIPS 2017       \n",
    "[arXiv](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951fdce-11e0-4da7-be72-8886704d8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"./Vaswani17a.pdf\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68149719-eb9a-485b-a939-d8b6d6888d20",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1139794-a640-4a21-a689-a80f6000ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning() # as we are using v2 below (which is in beta state)\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a261c1-dd15-4795-b47c-54b0a267298a",
   "metadata": {},
   "source": [
    "In the following, we will work through all the parts of the architecture (and the key concepts) on one example. Then, we will (1) consolidate all parts, tie them up in one class that implements the whole architecture, and (2) eventually train a ViT-based image classifier on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ae0d2-3225-44c8-9062-54c0c550f425",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb47e54-1bc0-48de-9da9-23bbbf12a88d",
   "metadata": {},
   "source": [
    "Lets load our favorite demo image first. We call this image $\\mathbf{x} \\in \\mathbb{R}^{H\\times W\\times C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf39e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/Users/rkwitt/Desktop/racoon.jpeg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d224e-76d9-43b8-8065-c86ecbe3a76e",
   "metadata": {},
   "source": [
    "### Patch extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea40b75-69fe-4e7e-8820-9e019400c898",
   "metadata": {},
   "source": [
    "When looking at Fig. 1 in [Dosovitskiy21a] (left), we see that we need to first split the image up into patches. These patches will essentially play the role of *tokens* in NLP tasks. \n",
    "\n",
    "For demonstration, we first scale our input image to $256 \\times 256$ pixel and write a transform that will give us a floating point tensor (float32) of size `BxCxWxH` where `B` is the batch size (here 1), `C` is the number of channels (here 3) and `W,H` will be 256 (as we just resized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14be48-f21f-47ce-9f88-5f07f170dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img256 = img.resize((256, 256)) # we resize here, but could do this as part of the transforms\n",
    "tf = v2.Compose([v2.PILToTensor(),\n",
    "                 v2.ToDtype(torch.float32)])\n",
    "img256 = tf(img256).unsqueeze(0)\n",
    "print(img256.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fccf94-5ea3-4602-aec2-224e47251628",
   "metadata": {},
   "source": [
    "Next, let us look at splitting the image into non-overlapping patches and linearly mapping each patch to a vector representation. The dimensionality of that representation in one parameter of the overall approach. In our demo case, we will split the image into `32x32` pixel patches. Overall, given the image size (here 256 times 256), we will have **64** patches overall. Upon vectorizing each patch (into a 1024-dim. vector), we will linearly map the patch to a 48-dim. representation space ($\\mathbb{R}^{48})$. \n",
    "\n",
    "Formally, as mentioned in Sec. 3.1 of [Dosovitskiy21a], we take $\\mathbf{x}$ of width $W$, height $H$ and $C$ channels and split the image into patches $\\mathbf{x}_p \\in \\mathbb{R}^{N\\times (P^2C)}$, where $P$ denotes the edge length of the (square) patches with $N=HW/P^2$. These patches are linearly/affinely mapped to $\\mathbb{R}^D$.\n",
    "\n",
    "To implement this step, we use `einops` to obtain the patches, `nn.Linear` for the linear/affine map and `nn.LayerNorm` to normalize the input and output to the linear layer. Here, the layer normalization (see [Ba16a](https://arxiv.org/abs/1607.06450)) (1) normalizes each of the $D$ dimensions by subtracting the mean and dividing by the standard deviation (computed over a minibatch), followed by a learnable scaling and shifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_width, patch_height = 32, 32\n",
    "patch_dim = patch_width*patch_height*3 # i.e., Nx(P^2*C)\n",
    "dim = 48\n",
    "\n",
    "emb_layer = nn.Sequential(\n",
    "    # see einops documentation https://einops.rocks/api/rearrange (and the PyTorch layer einops.layers.torch.Rearrange)\n",
    "    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', \n",
    "              p1 = patch_height, \n",
    "              p2 = patch_width),\n",
    "    nn.LayerNorm(patch_dim),\n",
    "    nn.Linear(patch_dim, dim), # map to R^D\n",
    "    nn.LayerNorm(dim))\n",
    "\n",
    "patch_emb = emb_layer(img256)\n",
    "batch_size, num_patches, _ = patch_emb.shape\n",
    "\n",
    "print('Batch size: {:d}'.format(batch_size))\n",
    "print('Number of patches: {:d}'.format(num_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c297ef-1ad2-4696-b59b-a04849e7a16d",
   "metadata": {},
   "source": [
    "As we can also see from Fig. 1 of [Dosovitskiy21a], we will also prepend an additional (learnable) representation that will encode the class token (this can later be used as an image representation once we obtain the transformer output, see below). So, overall, in our example, we have 64 vectors that come from linearly mapping the image patches **and** one vector that encodes the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31e580-eb90-45f7-bb5b-5dd30c6cf16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = torch.randn(1, 1, dim) # will be a nn.Parameter later (as we seek to learn it)\n",
    "cls_tokens = repeat(cls_token, '1 1 d -> b 1 d', b = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d587f5e-0820-4976-8bdc-10248c2c4bc0",
   "metadata": {},
   "source": [
    "Furthermore, we add a learnable **positional encoding**. This could be prescribed, but it can also be learned. Essentially, we are just adding another vector to each patch representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8961ff6-9642-49fd-92fc-de5fdebe49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = torch.randn(1, num_patches + 1, dim) # will be a nn.Parameter later\n",
    "\n",
    "# prepending the class token embedding\n",
    "x = torch.cat((cls_tokens, patch_emb), dim=1)\n",
    "# adding the position embedding\n",
    "x += pos_embedding[:,:(num_patches+1)]\n",
    "\n",
    "print(x.shape) # batch of size 1 with 65 embeddings of dimensionality 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9848ca-ea38-4a6e-b1ca-1e4bbda2d702",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "We now shift our focus to the right part of Fig. 1. that shows the **encoder** part of a transformer architecture. Input to this part are the 65 (in our example) vectors we obtained from the previous steps. Key to implementing this encoder, is to understand the **(multi-head) attention** module, which is an incarnation of the scaled dot-product attention mechanism from [Vaswani17a] (see paper linked above).\n",
    "\n",
    "Quoting from [Vaswani17a]: *An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.*\n",
    "\n",
    "In ViT's we have a special case of attention called **self attention** where keys, queries and values are the same. \n",
    "\n",
    "\n",
    "#### Single-Head Attention\n",
    "\n",
    "In the simplest case of only one attention head, we have three tensors: $\\mathbf{Q},\\mathbf{K},\\mathbf{V} \\in \\mathbb{R}^{B \\times M \\times D}$ which hold our patch embeddings. First, we multiple the queries with the keys\n",
    "\n",
    "$$\\mathbf{Q}\\mathbf{K}^\\top \\in \\mathbb{R}^{B \\times M \\times M}$$\n",
    "\n",
    "where the transpose $\\mathbf{K}$ is to be understood as transposing the last two dimensions. Upon scaling this tensor by $\\sqrt{D}$ and applying a $\\text{softmax}$ to the last dimension of the tensor, we obtain for each of the $M$ embeddings per batch element a $M$-dimensional probability vector, i.e.,\n",
    "\n",
    "$$ \\text{softmax}\\left( \\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{D}} \\right)\\in \\mathbb{R}^{B \\times M \\times M}$$\n",
    "\n",
    "Once we multiply the output of this operation with the \"values\" $\\mathbf{V}$, we get \n",
    "\n",
    "$$ \\text{softmax}\\left( \\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{D}} \\right)\\mathbf{V}\\in \\mathbb{R}^{B \\times M \\times D}$$\n",
    "\n",
    "In this manner, attention allows, per patch embedding, to incorporate information about all other patch embeddings, i.e., we can learn to attend to important/relevant parts of the image.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "The idea of multi-head attention is to not just use one attention function, but use multiple. In order to retain the dimensionality of the output $\\mathbb{R}^{B \\times M \\times D}$, we are not going to use $\\mathbf{Q},\\mathbf{K},\\mathbf{V}$ directly, but linearly project into a lower-dimensionalial space $d$ (in the paper, you have $d_k$ for keys and queries and $d_v$ for values). Each attention head gets its own projection matrix, i.e., for the $i$-th attention head we compute\n",
    "\n",
    "$$ \\text{softmax}\\left( \\frac{\\mathbf{Q}\\mathbf{W}_i^Q(\\mathbf{K}\\mathbf{W}^K_i)^\\top}{\\sqrt{d}} \\right)(\\mathbf{V}\\mathbf{W}_i^V)\\in \\mathbb{R}^{B \\times M \\times d}$$\n",
    "\n",
    "Concatenating the outputs of each attention head then yields the final output $\\in \\mathbb{R}^{B \\times M \\times Hd}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93736bc-bfa4-4150-85d7-b96f86d5a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "inner_dim = 64 *  num_heads\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "# chunk up into queries, keys and values\n",
    "qkv = to_qkv(x).chunk(3, dim=-1)\n",
    "\n",
    "# rearrange to account for multiple heads in the following matrix multiplication\n",
    "q,k,v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = num_heads), qkv)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2))\n",
    "attn = torch.softmax(dots, dim=-1)\n",
    "out = torch.matmul(attn, v)\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1a9ed-549b-48ba-a2b4-74366127df07",
   "metadata": {},
   "source": [
    "### Implementing the Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a62f04-2c18-497a-bf1a-bb21b9b4de40",
   "metadata": {},
   "source": [
    "Lets **clear** all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa90f92-ba59-4dea-a814-bb6197a9a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning() # as we are using v2 below (which is in beta state)\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57b7b7-34b1-49ab-91bd-015e4c812fb1",
   "metadata": {},
   "source": [
    "The following attention class implements the idea sketched above and also contains a final projection step, where we ensure that the output of the attention module (in case of multiple heads) is of dimensionality $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf3471-0552-44e7-b52b-6abedfc64afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim,            # dimension of input vectors\n",
    "                 heads = 8,      # number of attention heads\n",
    "                 dim_head = 64,  # internal dimensionality of each head\n",
    "                 dropout = 0.    # use dropout (in (0,1)) or not, i.e., 0.\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e11422-586d-4862-8e81-160d39472ce6",
   "metadata": {},
   "source": [
    "Next, we implement the MLP module from Sec. 3.1. of [Dosovitskiy21a]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf78930-682a-4214-8353-a7df977784d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05e4e0-ff9d-4438-a990-b45ab7c36c1e",
   "metadata": {},
   "source": [
    "We are now ready to implement the transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5b732-94ae-4299-9719-bfa23c1b7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e1406-7910-45f6-a1c4-89978ec2a0b6",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "Finally, we can combine all parts together. \n",
    "\n",
    "Note that the output of the final block in the transformer stack will yield an output of size \n",
    "$\\mathbb{R}^{B \\times M \\times D}$. To obtain **one** representation per batch element, we have multiple choices. The first choice is to use the representation of the class token, another choice is to average over all patch representations. In the code, we can select this \"pooling\" step wia `pool=cls` or `pool=mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f94ae7-8289-417f-b3ce-4597693ae1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6bbc3-ecff-4429-951b-03c1613d2447",
   "metadata": {},
   "source": [
    "## Training/Testing on CIFAR10\n",
    "\n",
    "For this demonstration, we do *not* pre-train on a substantially larger dataset and then finetune, but rather train on the CIFAR10 dataset directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96fe44-4290-4181-a687-fabb610e1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', \n",
    "           'car', \n",
    "           'bird', \n",
    "           'cat',\n",
    "           'deer', \n",
    "           'dog', \n",
    "           'frog', \n",
    "           'horse', \n",
    "           'ship', \n",
    "           'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29fdb9-cf0a-4ef6-9b3a-e79198135e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(image_size=32, \n",
    "          patch_size=8, \n",
    "          num_classes=10, \n",
    "          dim=128, \n",
    "          depth=4, \n",
    "          heads=8, \n",
    "          mlp_dim=128)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "import numpy as np\n",
    "print('#parameters', np.sum([s.numel() for s in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75993d8f-5992-471d-a9b5-7ce7ae03a6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa453c-61e1-41d6-be60-ef6f949e5995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
