{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298fa5de",
   "metadata": {},
   "source": [
    "# Computer Vision PS (WS21/22)\n",
    "\n",
    "\n",
    "# Exercise sheet E (ExE)\n",
    "\n",
    "**Group members**: please list all group members here\n",
    "\n",
    "**Total (possible) points**: tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee35839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e07ff9",
   "metadata": {},
   "source": [
    "## ExE.1 (2 points)\n",
    "\n",
    "Implement the binary cross entropy (BCE) loss using the provided function template (below). Here, `x` is a input tensor (i.e., a real number), `y` represents the target (i.e., 0 or 1). The function should return the result as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabf1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(x: tensor, y: tensor) -> tensor:\n",
    "    # YOUR CODE GOES HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ef38d",
   "metadata": {},
   "source": [
    "Test your code with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0235d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "x = torch.rand(1)\n",
    "y = tensor([1.])\n",
    "\n",
    "print(bce(torch.sigmoid(x),y))\n",
    "\n",
    "assert torch.abs(bce(torch.sigmoid(x),y) - \\\n",
    "                 F.binary_cross_entropy(torch.sigmoid(x),y))<1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01b86a",
   "metadata": {},
   "source": [
    "## ExE.2 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d32489",
   "metadata": {},
   "source": [
    "In general, the **cross-entropy** of two discrete probability distributions $p$ and $q$ with equal support $\\mathcal{S}$ is given by\n",
    "\n",
    "$$\n",
    "H(p,q) = - \\sum_{x \\in \\mathcal{S}} p(x)\\log q(x)\n",
    "$$\n",
    "Show that $H$ is convex in its second argument $q$. That is, we want to show that, for $0 \\leq \\lambda \\leq 1$, and $q_1,q_2$ discrete probability distributions with the same support as $p$ (i.e., the first argument to $H$), it holds that \n",
    "\n",
    "$$H(p,\\lambda q_1 + (1-\\lambda)q_2) \\leq \\lambda H(p,q_1) + (1-\\lambda)H(p,q_2)$$\n",
    "\n",
    "Use the fact that the cross-entropy has a close relationship to the **Kullback-Leibler divergence**, via\n",
    "\n",
    "$$ \\text{KL}(p \\Vert q)  = H(p,q) - H(p)\\enspace,$$\n",
    "\n",
    "where $H(p)$ is nothing else than the **entropy** of $p$, i.e.,\n",
    "\n",
    "$$H(p) = -\\sum_{x \\in \\mathcal{S}} p(x)\\log p(x)\\enspace.$$\n",
    "\n",
    "The Kullback-Leibler divergence is formally defined as\n",
    "\n",
    "$$\\text{KL}(p \\Vert q) = -\\sum_{x \\in \\mathcal{S}} p(x)\\log \\frac{p(x)}{q(x)}\\enspace.$$\n",
    "\n",
    "We also **do know** that $\\text{KL}(p\\Vert q)$ is **convex** in $(p,q)$, i.e.,\n",
    "\n",
    "$$ \\text{KL}(\\lambda p_1 + (1-\\lambda)p_2 \\Vert \\lambda q_1 + (1-\\lambda)q_2) \\leq \\lambda \\text{KL}(p_1\\Vert q_1) + (1-\\lambda)\\text{KL}(p_2\\Vert q_2)$$\n",
    "\n",
    "Complete the argument ... (in a new Jupyter notebook cell, using Markdown syntax as used in this cell).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65b9c9",
   "metadata": {},
   "source": [
    "## ExE.3 (3 points)\n",
    "\n",
    "When training a classifier, the most natural loss for training that would come to mind is the **0-1 loss** (as it is the loss you use to eventually assess your model's performance) , i.e., the **indicator function** indicating whether our prediction is correct (i.e., zero loss), or incorrect (i.e., loss of 1). However, using gradient descent based optimization would not work, as the 0-1 loss is not differentiable.\n",
    "\n",
    "Provide a visual explanation (i.e., a plot and maybe 1-2 sentences) as to why using the binary cross entropy loss is a good idea (aside from its probabilistic interpretation that we know from the lecture).  \n",
    "\n",
    "**Hint**: Imagine you have a binary classification problem (with labels $\\{0,1\\}$) and your model outputs a scalar (i.e., $\\in \\mathbb{R}$). If the output is negative, you assign label $0$ and $1$ otherwise. Assuming the true label of a data point is $1$, and your model outputs a negative value, the 0-1 loss would (obviously) return a loss of $1$ (plot this). Now, translate this setting into the binary cross entropy setting (and also plot the result). Then interpret the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2235f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# <YOUR CODE GOES HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892ba6b",
   "metadata": {},
   "source": [
    "Your explanation goes here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
