{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "## <font color='blue'>Tensor 101 (Tensor basics)</font>\n",
    "\n",
    "---\n",
    "\n",
    "This introduction only provides the very basics of working with tensors (in **PyTorch**). For a more comprehensive overview (and introduction), see the PyTorch [documentation](https://pytorch.org/docs/stable/index.html) and the [tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- [Tensor types](#Tensor-types)\n",
    "- [Writing device agnostic code](#Writing-device-agnostic-code)\n",
    "- [Tensor creation and filling](#Tensor-creation-and-filling)\n",
    "- [In-place operations](#In-place-operations)\n",
    "- [Reduction operations](#Reduction-operations)\n",
    "- [Indexing](#Indexing)\n",
    "- [Slicing](#Slicing)\n",
    "- [Views](#Views)\n",
    "- [Expansion](#Expansion)\n",
    "- [Images as tensors](#Images-as-tensors)\n",
    "- [Operations](#Operations)\n",
    "    - [Matrix-vector products](#Matrix-vector-products)\n",
    "    - [Matrix-matrix products](#Matrix-matrix-products)\n",
    "- [Interface to standard linear operations](#Interface-to-standard-linear-operations)    \n",
    "    - [Solving linear least-squares regression](#Solving-linear-least-squares-regression)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, a tensor is a *generalized matrix*. A ...\n",
    "\n",
    "- ... 0d tensor is a scalar\n",
    "- ... 1d tensor is a vector\n",
    "- ... 2d tensor is a matrix\n",
    "- ... 3d tensor can be interpreted as a vector of identically-sized matrices\n",
    "- ... 4d tensor can be interpreted as a matrix of identically-sized matrices\n",
    "- etc.\n",
    "\n",
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.rand(1)        # scalar\n",
    "t1 = torch.rand(1,10)     # 1x10 vector\n",
    "t2 = torch.rand(5,10)     # 5x10 matrix \n",
    "t3 = torch.rand(3,5,10)   # vector of length 3 of 3x5 matrices\n",
    "t4 = torch.rand(4,3,5,10) # 4x3 matrix of 5x10 matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use tensors to <font color='blue'>encode</font> the input/output signals, intermediate outputs, and also the parameters (internal states) of our models. \n",
    "\n",
    "In **PyTorch**, we have efficient tensor operations available on CPU/GPU, automatic differentiation (will be discussed later), etc. If a tensor is stored in memory on a certain device (GPU/CPU), operations are performed by that device.\n",
    "\n",
    "## Tensor types\n",
    "\n",
    "We have the following data types available for tensors:\n",
    "\n",
    "- `torch.float16`\n",
    "- `torch.float32`\n",
    "- `torch.float64`\n",
    "- `torch.uint8`\n",
    "- `torch.int8`\n",
    "- `torch.int16`\n",
    "- `torch.int32`\n",
    "- `torch.int64`\n",
    "\n",
    "For `torch.float32`, the corresponding CPU tensor is `torch.FloatTensor` and the corresponding GPU tensor is `torch.cuda.FloatTensor`.\n",
    "\n",
    "For a full listing see [here](https://pytorch.org/docs/stable/tensors.html). \n",
    "\n",
    "Note that `torch.Tensor` defaults to `torch.FloatTensor`. The preferred construction of tensors happens via `torch.tensor`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "x = torch.Tensor()\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing device agnostic code\n",
    "\n",
    "In some cases, we might want to run our code on CPU, in other cases we might want to run it on the GPU. Writing device-agnostic code is simple in PyTorch. We just set the `device` variable to the correct device that we have available. \n",
    "\n",
    "Note that if you have multiple GPUs, we can directly specify the GPU ID via `cuda:x`, e.g., `cuda:2`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "if torch.cuda.is_available() and use_gpu: \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tensor creation and filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty 8x8 tensor (i.e., a 8x8 matrix)\n",
    "x = torch.empty(8, 8, \n",
    "                dtype=torch.float64, \n",
    "                device=device)\n",
    "\n",
    "print(x)\n",
    "print('Tensor type:', x.dtype)\n",
    "print('Device:', x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have *uninitialized data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x2 tensor filled with zeros\n",
    "x = torch.zeros(3, 2, \n",
    "                device=device) \n",
    "print(x)\n",
    "print('Tensor type', x.dtype)\n",
    "print('Device:', x.device)\n",
    "\n",
    "# tensor of same size as x, filled with zeros\n",
    "y = torch.zeros_like(x).to(device) \n",
    "print('Tensor type:', y.dtype)\n",
    "print('Device:', y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3x2x10 tensor (i.e., 3 matrices of size 2x10) of all ones\n",
    "x = torch.ones(3, 2, 10, \n",
    "               device=device)\n",
    "print(x)\n",
    "print(x.size())\n",
    "print('Tensor type:', x.dtype)\n",
    "print('Device:', x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create tensors from numpy arrays. Unless we specify the tensor type directly, the `numpy` array type is inherited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        [1.,2.,3.],\n",
    "        [4.,5.,6.]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# create tensor from numpy array\n",
    "t = torch.tensor(x, device=device)\n",
    "print(t)\n",
    "print(t.dtype)\n",
    "\n",
    "x = np.array(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6]\n",
    "    ])\n",
    "t = torch.tensor(x, \n",
    "                 dtype=torch.float32,\n",
    "                 device=device)\n",
    "print(t)\n",
    "print(t.dtype)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## In-place operations\n",
    "\n",
    "Operations that operate directly on the tensor and *do not* make a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3, 1, device=device, dtype=torch.float32)\n",
    "y = torch.empty(3, 1, device=device, dtype=torch.float32)\n",
    "z = torch.empty(3, 1, device=device, dtype=torch.float32)\n",
    "\n",
    "x.fill_(1)  # filled with ALL ones\n",
    "y.zero_()   # filled with ALL zeros\n",
    "z.normal_() # filled with elements drawn from N(0,1) - i.e., a standard Gaussian\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In-place operations are post-fixed with a `_`, such as `x.zero_()` or `x.fill_(1.0)` or `x.normal_()`. The latter fills the tensor with values sampled from a standard Gaussian $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reduction operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(3, 2).normal_()\n",
    "print(x)\n",
    "\n",
    "s0 = x.sum()  # sum over all tensor values\n",
    "s1 = x.mean() # mean over all tensor values\n",
    "s2 = x.std()  # computed the standard deviation over all tensor values\n",
    "s3 = x.norm() # matrix norm of x (in that case Frobenius norm)\n",
    "\n",
    "print(s0.item())\n",
    "print(s1.item())\n",
    "print(s2.item())\n",
    "print(s3.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that While `s0` ... `s3` are scalars, they are encoded as a 0d tensor. To get a Python number, we call `.item()`.\n",
    "\n",
    "We can also perform these operations over specific dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10,1,10)\n",
    "\n",
    "# x contains 10 1x10 vectors - lets sum over the elements of each vector\n",
    "sum_v0 = x.sum(dim=2)\n",
    "# check first element\n",
    "sum_v1 = x[0,...].sum()\n",
    "\n",
    "assert sum_v0[0] == sum_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Indexing\n",
    "\n",
    "Indexing starts at 0 (as usual in our context). Say you have a $3 \\times 2$ tensor `x`. The dimensionality along the first dimension is $3$ and along the second dimension $2$. To get the value at $(i,j)$, we specify\n",
    "`x[i,j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 2, 2, \n",
    "               device=device)\n",
    "print(x)\n",
    "print(x[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3, 10, 12, \n",
    "               device=device).normal_()\n",
    "print(x[0,8,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract certain parts of a tensor, using *slicing*, e.g., if `x = torch.rand(3,10,12)` as before, we have $3$ $10 \\times 12$ matrices in this tensor, or, a vector (of length 3) of $10 \\times 12$ matrices. To extract the first vector element (i.e., the first $10 \\times 12$ matrix), we write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,2,2)\n",
    "print(x.size())\n",
    "z = x[0,:,:]    # equivalent to x[0,...]\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If we modify a value in `z`, we also modify the value in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Without cloning ...')\n",
    "x = torch.rand(1,2)\n",
    "print('x (original):', x)\n",
    "z = x\n",
    "z[0,0] = 1.\n",
    "print('x (original, after modification of z):', x)\n",
    "print('z:', x)\n",
    "print()\n",
    "\n",
    "x = torch.rand(1,2)\n",
    "print('With cloning ...') \n",
    "print('x (original):', x)\n",
    "z = x.clone()\n",
    "z[0,0] = 1.\n",
    "print('x (original, after modification of z):', x)\n",
    "print('z:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing also allows to set all values of a slice simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(3, 1, 2, \n",
    "               device=device).normal_()\n",
    "\n",
    "x[0,:,:] = 2\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "x[0,:,:].zero_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Views\n",
    "\n",
    "We can also easily create different *views* of a tensor. A common procedure is to view the tensor as a vector, which can easily be done via: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "print(x.size())\n",
    "print()\n",
    "\n",
    "# 1d view\n",
    "v0 = x.view(-1)\n",
    "print(v0)\n",
    "print()\n",
    "\n",
    "# view as 3x2 tensor\n",
    "v1 = x.view(3,-1)\n",
    "print(v1)\n",
    "print()\n",
    "\n",
    "# ... or more explicitely\n",
    "v2 = x.view(3, 2)\n",
    "print(v2)\n",
    "print()\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`v0` now is a 1d view of the tensor `x`. Again, modifying `v0` modifies `x`. Also, the original size of `x` does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Expansion\n",
    "\n",
    "*Expansion* expands singleton dimensions to a larger size (by creating a new view *without* memory consumption). In the next example we have a $2 \\times 3$ tensor and want to expand this one to a $3 \\times 2 \\times 3$ tensor, i.e., expansion along the first singleton dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6]\n",
    "    ])\n",
    "\n",
    "print(x.size())\n",
    "print()\n",
    "\n",
    "y = x.expand(5,2,3)\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `unsqueeze` we can also expand tensors by adding ONE singleton dimension. Below is an example of unsqueezing along dimension 0. The opposite operation is `squeeze`, obviously :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6]\n",
    "    ])\n",
    "x = x.unsqueeze(0)\n",
    "print(x.size())\n",
    "x = x.squeeze()\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Images as tensors\n",
    "\n",
    "Images are just tensors ... it's that simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# download a popular computer vision dataset\n",
    "cifar = torchvision.datasets.CIFAR10(\n",
    "    '../data/cifar10/', \n",
    "    train = False,\n",
    "    download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cifar` is an instance of `torchvision.dataset`. We can access it's elements via `cifar[idx]` and get a tuple of `(data, label)`. `data` is a `PIL` image, `label` should be an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cifar[600][0]\n",
    "lab = cifar[600][1]\n",
    "print(type(img))\n",
    "print(type(lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, `cifar` has a `data` class variable which holds all images in a numpy array. Lets convert this into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cifar.data.shape)\n",
    "\n",
    "# we use permute to switch dimensions, as originally images are in the format channels x W x H\n",
    "x = torch.from_numpy(cifar.data).permute(0, 3, 1, 2).float()\n",
    "print(x.type())\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using the `torchvision` module, we have just loaded *all* images from the CIFAR-10 testing set. CIFAR-10 are RGB images (hence 3 color channels) and we have 10,000 images available. Consequently, we get a $10000 \\times 3 \\times 32 \\times 32$ tensor.\n",
    "\n",
    "This is the convention for multi-channel images in PyTorch, i.e., nr. of images times channels times width times height. For visualization using `matplotlib` we need to permute the channels, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(\n",
    "        np.transpose(npimg, (1, 2, 0)), \n",
    "        interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(131)\n",
    "show(x[100,:,:,:]/255.)\n",
    "plt.subplot(132)\n",
    "show(x[200,:,:,:]/255.)\n",
    "plt.subplot(133)\n",
    "show(x[300,:,:,:]/255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also has its own convenience methods to quickly visualize images in a tensor on a grid (using `torchvision.utils.make_grid`. E.g., we can narrow the tensor containing all CIFAR10 testing images along its first dimension to only contain 48 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "y = x.narrow(0, 0, 48).float()\n",
    "collage = make_grid(y, normalize=True)\n",
    "plt.figure(figsize=(20,10))\n",
    "show(collage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication\n",
    "x = torch.ones(3,1)*2\n",
    "y = torch.ones(3,1)*4\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x*y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squaring\n",
    "x = torch.ones(3,1)*2.\n",
    "print(x**2)\n",
    "print(torch.pow(x,2.)) # equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,0]\n",
    "    ])\n",
    "y = torch.tensor([1,2,3])\n",
    "M.mv(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-matrix products\n",
    "\n",
    "Matrix-matrix products are written via `@`. For a 2d tensor `M`, we can also equivalently write `M.mm(M)` to compute the matrix multiplication of `M` with itself, or `torch.mm(M,M)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M @ M)         # Variant 1\n",
    "print(M.mm(M))       # equivalent to Variant 1 \n",
    "print(torch.mm(M,M)) # equivalent to Variant 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interface to standard linear operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we can use `torch.lstsq` as a **least-squares** problem solver. Say you want to solve\n",
    "\n",
    "$$ \n",
    " \\min_{\\mathbf{X}}  \\|\\mathbf{A}\\mathbf{X}-\\mathbf{B}\\|_2.\n",
    "$$\n",
    "\n",
    "for a given full-rank matrix $\\mathbf{A}$ ($m \\times n$) and a matrix $\\mathbf{B}$ ($m \\times k$). \n",
    "\n",
    "In the following **example**, $\\mathbf{B}$ is of size $3 \\times 1$ and $\\mathbf{A}$ is of size $3 \\times 3$. Both matrices will be filled with values from a standard Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.empty(3,1).normal_()   # 2d tensor of size 3x1 (i.e., a vector)\n",
    "A = torch.empty(3,3).normal_()   # 2d tensor (matrix)\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "# solve min_x ||Ax - b||_2\n",
    "x, _ = torch.lstsq(B,A) \n",
    "\n",
    "# some arbitrary vector just as a sanity check\n",
    "z = torch.rand(3,1)\n",
    "\n",
    "# Note: both x and z are 3x1 matrices (2d) tensors representing vectors\n",
    "# Hence, we use mm() to compute matrix-matrix products.\n",
    "\n",
    "Ax = A.mm(x)  \n",
    "Az = A.mm(z) \n",
    "\n",
    "print('Norm (using x): {:.5f}'.format(torch.norm(Ax.view(3,) - B.view(3,)).item()))\n",
    "print('Norm (using z): {:.5f}'.format(torch.norm(Az.view(3,) - B.view(3,)).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving linear least-squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given \n",
    "\n",
    "$$(x_1,y_1), \\ldots, (x_N,y_N)$$\n",
    "\n",
    "with $(x_i,y_i) \\in \\mathcal{Z} = \\mathbb{R} \\times \\mathbb{R}$ and want to fit the best-fitting **line** through this data (in a least-squares sense), i.e., a model of the form\n",
    "\n",
    "$$ f(x;a,b) = ax + b$$\n",
    "\n",
    "This linear least squares **optimization problem** (in the parameters $a,b$) can be written as\n",
    "\n",
    "$$ \\arg\\min_{a,b} \\frac{1}{N} \\sum_{i=1}^N (ax_i +b -y_i)^2$$\n",
    "\n",
    "i.e., we try to minimize the sum of squared residuals $(ax_i +b -y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon obtaining a solution, we can put in a (previously unseen) $x^*$ and predict the corresponding value $y^*$ via \n",
    "\n",
    "$$f(x^*; a,b)=\\hat{y}$$\n",
    "\n",
    "The toy data we use is taken from the [Isotonic Regression](https://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html#sphx-glr-auto-examples-plot-isotonic-regression-py) example from `sklearn` (to find a non-decreasing approximation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "N = 100\n",
    "x = np.arange(N)\n",
    "rs = check_random_state(0)\n",
    "y = rs.randint(-50, 50, size=(N,)) + 50. * np.log1p(np.arange(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `np.log1p(x)` returns $\\log(1+x)$ and the toy data can obviously not modeled by a linear function - However, for our purposes this is *linear enough* as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'r.')\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n",
    "plt.title('Toy data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the problem**    \n",
    "\n",
    "First, we construct our $N \\times 2$ data matrix $\\mathbf{A}$ as follows:\n",
    "\n",
    "$$\\mathbf{A} = \\left(\n",
    "\\begin{matrix}\n",
    "x_1 & 1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_N & 1\n",
    "\\end{matrix}\n",
    "\\right)$$\n",
    "\n",
    "Note the additional column of all 1's - this is to include the offset (b).\n",
    "We put our targets $y_i$ in a $N \\times 1$ matrix (i.e., a colum vector)\n",
    "\n",
    "$$\\mathbf{B} = \\left(\n",
    "\\begin{matrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{matrix}\n",
    "\\right)$$\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "x_1 & 1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_N & 1\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "\\left(\\begin{matrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\approx\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Hence, encoding $\\mathbf{X} = [a,b]^\\top$, we want to solve\n",
    "\n",
    "$$ \\min_{\\mathbf{X}} \\frac{1}{N} \\| \\mathbf{A}\\mathbf{X} - \\mathbf{B} \\|_2^2$$\n",
    "\n",
    "Note that this minimization problem is equivalent to \n",
    "\n",
    "$$ \\min_{\\mathbf{X}} \\| \\mathbf{A}\\mathbf{X} - \\mathbf{B} \\|_2$$\n",
    "\n",
    "as square-rooting the norm is a monotonic transformation (i.e., it does not matter which one we minimize). The latter, however, fits our setting of the `torch.gels()` function. Similarly, the missing $1/N$ term does not change the optimization result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.empty(N, 2, dtype=torch.float)\n",
    "data_y = torch.empty(N, 1, dtype=torch.float)\n",
    "\n",
    "data_x[:,0] = torch.tensor([x])\n",
    "data_x[:,1] = 1.\n",
    "data_y[:,0] = torch.tensor([y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example `data_x` acts as our matrix $\\mathbf{A}$, and `data_y` acts as our matrix $\\mathbf{B}$. $\\mathbf{A}$ is of size $(m,n) = (N,2)$, $\\mathbf{B}$ of size $(n,k) = (N,1)$. According to the documentation the returned solution has size $(\\max(m,n), k)$, so we will get a matrix of size $(100,1)$ back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, _ = torch.lstsq(data_y, data_x)\n",
    "print(alpha.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, according to the documentation, the first $n$ (in our case $n=2$) rows contain the solution, the remaining $100-2$ entries contain the residual sum of squares for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = alpha[0,0].item()\n",
    "b = alpha[1,0].item()\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick comparison to the actual isotonic regression example in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "ir = IsotonicRegression()\n",
    "y_ = ir.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, y, 'r.', label='Data')\n",
    "plt.plot(x, a*x + b, 'black', label='Fit')\n",
    "plt.plot(120, a*120+b, '+', label='Prediction @ x=120')\n",
    "plt.plot(x, y_, 'g.-', label='Isotonic fit')\n",
    "plt.legend();\n",
    "plt.title('Linear least-squares fitting');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,y_,'.',color='red', label='Isotonic fit (r2={:.2f})'.format(r2_score(y, y_)))\n",
    "plt.plot(y,a*x+b,'.',color='black', label='Linear least squares (r2={:.2f})'.format(r2_score(y, a*x+b)))\n",
    "plt.xlabel('True $y$');\n",
    "plt.ylabel('Prediction');\n",
    "plt.legend();\n",
    "plt.title(r'$R^2$ comparison (closer to 1 is better)', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Behavior of `x[0,::]` vs. `x[::,0]`\n",
    "\n",
    "**Answer**: I mistyped the *ellipsis* operation, which is `...` **not** `::`. So, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(2,3,4)\n",
    "print(x[...,0] == x[:,:,0]) # equivalent\n",
    "print(x[0,...] == x[0,:,:]) # equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least-squares question**: `torch.gels` is deprecated; use `torch.lstsq` instead. I think this was an artifact of the earlier days of `PyTorch`. `torch.lstsq` is just a **least-squares solver** and has nothing to do with generalized least squares (used when there is correlation between the residuals). \n",
    "\n",
    "Somehow, I had the `statsmodels` package behavior in mind, my mistake!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
