{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "Roland Kwitt, 2020\n",
    "\n",
    "## <font color='blue'>Automatic differentiation (aka AutoGrad)</font>\n",
    "\n",
    "In this lecture, we take a look at the mechanics of **automatic differentiation** (aka *AutoGrad* / *AutoDiff*) in PyTorch. These techniques are at the very heart of today's frameworks for learning with neural networks.\n",
    "\n",
    "You can also find the official (tutorial-like) documentation [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introductory example](#Introductory-example)\n",
    "- [A  slightly more involved example](#A-slightly-more-involved-example)\n",
    "- [The AutoGrad machinery](#The-AutoGrad-machinery)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import agtree2dot\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from utils import visualize_DAG\n",
    "from IPython.display import HTML\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introductory example\n",
    "\n",
    "In this example, we take the following function\n",
    "\n",
    "$$f(x;w,b) = \\max\\{wx + b, 0\\} = \\text{ReLU}(wx+b)$$\n",
    "\n",
    "and first look at its representation as a **computation graph** (with example values for $w,b$ and $x$ added):\n",
    "\n",
    "<img src=\"DAG0.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute  \n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial b} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial b}$$\n",
    "\n",
    "First, \n",
    "\n",
    "$$ \\frac{\\partial v}{\\partial b} = 1 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial v} = \\frac{\\partial}{\\partial v} \\text{ReLU}(v) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}~v \\leq 0\\\\\n",
    "1 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hence, in our case we have\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial v} = 1$$\n",
    "\n",
    "and consequently \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "Let's try to compute the partial derivative of $a$ wrt. $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w}\n",
    "$$\n",
    "\n",
    "So, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial v}{\\partial u} = 1\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial w} = x\n",
    "$$\n",
    "\n",
    "Combined, we obtain (if we use the fact that $x=3$)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w} = 1\\cdot 1\\cdot 3 = 3\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Conceptually, the **forward pass** (i.e., the computation of the function value) is a sequence of standard tensor computations (i.e., in this example simply the computation of all intermediate results), and the **Directed Ascyclic Graph (DAG)** of tensor operations is required only to compute derivatives (via the chain rule). \n",
    "\n",
    "When executing tensor operations, PyTorch can automatically (on-the-fly) construct the graph of operations to compute the gradient of any quantity with respect to any tensor involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.], grad_fn=<ClampBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u = x*w\n",
    "v = u+b\n",
    "a = torch.clamp(v, min=0) # alternatively: a = F.relu(x*w + b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensor has a Boolean field `requires_grad`, set to `False` by default, which\n",
    "states if PyTorch should build the graph of operations so that gradients with\n",
    "respect to it can be computed.\n",
    "\n",
    "**Note**: Only *floating point type* tensors can have their gradient computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = 7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"v =\", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "grad_w = grad(a, w, retain_graph=True)\n",
    "print(grad_w[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(a, b)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A slightly more involved example\n",
    "\n",
    "In this example, we have a slightly more complex computation graph with a *shared* weight $w$ and two paths to arrive at the result. The function is\n",
    "\n",
    "$$f(x;w) = (wx)^2 + (wx)^3$$\n",
    "\n",
    "<img src=\"DAG1.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the partial derivative of $a$ wrt. $w$ by hand once more:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v_1}\\frac{\\partial v_1}{\\partial u_1}\\frac{\\partial u_1}{\\partial w} + \\frac{\\partial a}{\\partial v_2}\\frac{\\partial v_2}{\\partial u_2}\\frac{\\partial u_2}{\\partial w} = \n",
    "1\\cdot 2u_1\\cdot x + 1 \\cdot 3u_2^2 \\cdot x = 8 + 24 = 32\n",
    "$$\n",
    "\n",
    "Again, having **all the intermediate values** during the **forward pass** makes it very easy to actually compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 12.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.])\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u1 = x*w\n",
    "u2 = x*w\n",
    "\n",
    "v1 = torch.pow(u1, 2)\n",
    "v2 = torch.pow(u2, 3)\n",
    "\n",
    "a = v1 + v2\n",
    "print(\"a =\", a.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(a, w)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The AutoGrad machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd DAG is encoded through the fields `grad_fn` of Tensors, and the\n",
    "fields `next_functions` of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# below is another way of saying requires_grad=True\n",
    "x = torch.tensor([ 1.0, -2.0, 3.0, -4.0 ]).requires_grad_() \n",
    "a = x.pow(2.)\n",
    "s = a.sum()\n",
    "print(s.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize a simple computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 2.]).requires_grad_()\n",
    "q = x.norm() # L2-norm of tensor sqrt(1^2+2^2+2^2)=3\n",
    "print(q.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"simple1.jpg\" align=\"center\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agtree2dot.save_dot(q, {x: 'x', q: 'q'}, open('simple1.dot', 'w'))\n",
    "HTML(visualize_DAG('simple1.dot', 'simple1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another, slightly more involved, example: We have $\\mathbf{x} \\in \\mathbb{R}^{10}$ and **first** compute (noting that $\\text{tanh}$ is applied componentwise).\n",
    "\n",
    "$$\n",
    "\\mathbf{h}  = \\text{tanh}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) \\\\\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_1 \\in \\mathbb{R}^{20 \\times 10}$ and $\\mathbf{b}_1 \\in \\mathbb{R}^{20}$. This results in $\\mathbf{h} \\in \\mathbb{R}^{20}$. **Second**, we compute\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}}  = \\text{tanh}(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2)\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_2 \\in \\mathbb{R}^{5 \\times 20}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^5$. This results in \n",
    "$\\hat{\\mathbf{y}} \\in \\mathbb{R}^5$. And **finally**, we compute (the mean-squared error)\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y},\\hat{\\mathbf{y}}) = \\frac{1}{5} \\sum_{i=1}^5(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i,\\hat{y}_i$ are the $i$-th elements of $\\mathbf{y}$ and $\\hat{\\mathbf{y}}$, respectively. In the example below, $y_i$ (which we use as targets) are just random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6597943305969238\n"
     ]
    }
   ],
   "source": [
    "w1 = torch.rand(20, 10).requires_grad_()\n",
    "b1 = torch.rand(20).requires_grad_()\n",
    "w2 = torch.rand(5, 20).requires_grad_()\n",
    "b2 = torch.rand(5).requires_grad_()\n",
    "\n",
    "x = torch.rand(10)\n",
    "h = torch.tanh(w1 @ x + b1)\n",
    "yhat = torch.tanh(w2 @ h + b2)\n",
    "\n",
    "y = torch.rand(5)\n",
    "\n",
    "loss = (y - yhat).pow(2).mean()\n",
    "loss.backward() # computes the partial derivatives wrt. w1,b1,w2,b2\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the computation graph ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"simple2.jpg\" align=\"center\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agtree2dot.save_dot(loss, \n",
    "                    {\n",
    "                        w1: 'w1',\n",
    "                        b1: 'b1',\n",
    "                        w2: 'w2',\n",
    "                        b2: 'b2',\n",
    "                        loss: 'loss'\n",
    "                    }, open('simple2.dot', 'w'))\n",
    "HTML(visualize_DAG('simple2.dot', 'simple2.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `torch.no_grad()`\n",
    "\n",
    "The `torch.no_grad()` **context** switches off the autograd machinery, and can be\n",
    "used for operations such as parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333333 -0.333333\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( 0.7).requires_grad_()\n",
    "b = torch.tensor(-0.7).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a - b)**2 # i.e., the forward pass\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "\n",
    "    # walk towards the direction of the negative gradient (aka gradient descend)\n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `detach()`\n",
    "\n",
    "The `detach()` method creates a tensor which shares the data, but does not\n",
    "require gradient computation, and is not connected to the current graph.\n",
    "\n",
    "This method should be used when the gradient should not be propagated\n",
    "beyond a variable, or to update leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = x**3\n",
    "r=(y+z).sum()\n",
    "r.backward() # computes the gradient of r wrt. x\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets exclude the contribution of the $x^3$ branch via `detach()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20., grad_fn=<SumBackward0>)\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = (x**3).detach()\n",
    "r=(y+z).sum()\n",
    "print(r)\n",
    "r.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to our exampe of finding the minimum of a function (from above) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000 -0.000000\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( 0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a.detach() - b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
